---
title: "PresentationWritten"
output: html_document
---


Motivation: Be sure to motivate your topic at the beginning of your write-up. You should try to hook the reader early on. Assume that your audience is a skeptical data scientist who has stumbled across your blog but has very little time to read it. Can you give her a reason to continue reading? A cool visualization or result can help.




Google BigQuery: We used Google's BigQuery to source our datasets. Since we used Person, Vehicle and Accident level data, BigQuery provided a cohesive and structured environment for managing all three. Additionally the user-friendly interface was conducive to basic exploratory analysis with SQL as well as for performing variety of joins. BigQuery as a service is an analytics warehouse with the ability to process data on the petabyte scale. It provides a serverless and infrastructure-less environment since it deviates the need of a database administrator given that the data is processed and stored on the cloud. Its features include the ability to ingest data from different sources including Google Cloud Storage, Cloud Datastore, and livestream. Data can be read and written via Cloud Dataflow, Spark, and Hadoop and exported out in the Cloud. A key feature of BigQuery is the ability to collaborate and share queries as well as data by adding members to a project.
  Most basic and preliminary use of BigQuery entails navigating between two environments: Google BigQuery and Google Cloud PLatform. Data can be uploaded in BigQuery by first creating a project from the ProjectsPage and enable billing as well as the BigQuery API. Once a project has been created, it can be selected on the Google BigQuery platform and datasets can be added from the create a new dataset drop down option on the highlighted project (available on the left side of the inteface). After specifying a dataset, it is populated by tables of interest. The specification of the table entails defining a schema (structure or data skeleton), which means specifying the variable names as well the data types for each variable. The variable names and data types should match the original data file that is being exported to BigQuery. Once the data is exported in a table, it can be previewed and queried through the compose query tab on the left. Tables can additionally be joined as well given specification of a unique key. Past query and job history can be viewed on the left. 
  After the data is queried, the resulting dataset can be exported out to the Cloud. This export can be achieved by first creating a bucket from the Cloud console. Buckets can be created by selecting the Storage option from the tabbed main Cloud Platform console page. After a bucket is created, the queried data can be exported to that bucket with file name and format specified. 
  BigQuery has additional features worth highlighting like the publicly available datasets, which include the National Oceanic and Atmospheric Administration (NOAA) global data obtained from the USAF Climatology Center, US Disease Surveillance data from the CDC, NYC Taxi and Limousine Commission (TLC) Trip Data and GDELT Internet Archive Book Data. 



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(acs)
library(tibble)
library(dplyr)
library(mlbench)
library(caret)
library(readr)
require(mosaic)
library(plyr)
library(randomForest)
library(geepack)
library(gee)
```

```{r}
#Rejoining the state and county datasets to create a collective dataframe 
fips.county <- rename(fips.county, c("State" = "STATEL"))
fips.state <- rename(fips.state, c("STUSAB" = "STATEL"))
fips.county$STATEL <- as.factor(fips.county$STATEL)
fips.state$STATEL <- as.factor(fips.state$STATEL)
join <- inner_join(fips.state, fips.county, by = "STATEL")

fipsEncode <- join %>% mutate(FIPSCode = ((1000*State.ANSI) + County.ANSI))
fipsEncode1 <- fipsEncode %>% mutate(StateCounty = paste(County.Name, STATE_NAME, sep = ", "))

#TOTAL POPULATION 
x <- geo.make(state = "*", county = "*")
y <- acs.fetch(endyear=2015, geography=x, table.number="B01003", 
               key = "17b6e09794a8f4a42664535f0e519179cc06f5a7")
z <- estimate(y)
zdata <- as.data.frame(z)
dfPop <- tibble::rownames_to_column(zdata, "StateCounty")
JoinPop <- inner_join(dfPop, fipsEncode1, by = "StateCounty")

#INCOME TO POP RATIO  
x1 <- geo.make(state = "*", county = "*")
y1 <- acs.fetch(endyear=2015, geography=x1, table.number="C17002", key = "17b6e09794a8f4a42664535f0e519179cc06f5a7")
z1 <- estimate(y1)
z1 <- as.data.frame(z1)
z2 <- z1 %>% dplyr::select(C17002_001)
dfIncome <- tibble::rownames_to_column(z2, "StateCounty")
dfTotData <- inner_join(dfIncome, JoinPop, by = "StateCounty")

save(dfTotData, file = "dfTotData.csv")
```
