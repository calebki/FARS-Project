---
title: "PresentationWritten"
output: html_document
---


Motivation: Be sure to motivate your topic at the beginning of your write-up. You should try to hook the reader early on. Assume that your audience is a skeptical data scientist who has stumbled across your blog but has very little time to read it. Can you give her a reason to continue reading? A cool visualization or result can help.


Google BigQuery: We used Google's BigQuery as a hosting platform for our datasets. BigQuery as a service is an analytics warehouse with the ability to process data on the petabyte scale. It provides a serverless and infrastructure-less environment since it deviates the need of a database administrator given that the data is processed and stored on the cloud. Its features include the ability to ingest data from different sources including Google Cloud Storage, Cloud Datastore, and livestream. Data can be read and written via Cloud Dataflow, Spark, and Hadoop and exported out in the Cloud. A key feature of BigQuery is the ability to collaborate and share queries as well as data by adding members to a project. Since we used Person, Vehicle and Accident level data, BigQuery provided a cohesive and structured environment for managing all three. Additionally the user-friendly interface was conducive to basic exploratory analysis with SQL as well as for performing variety of joins. 

  Most basic and preliminary use of BigQuery entails navigating between two environments: Google BigQuery and Google Cloud PLatform. Data can be uploaded in BigQuery by first creating a project from the ProjectsPage and enable billing as well as the BigQuery API. Once a project has been created, it can be selected on the Google BigQuery platform and datasets can be added from the 'create a new dataset drop down option' on the highlighted project (available on the left side of the inteface). After specifying a dataset, it is populated by tables of interest. The specification of the table entails defining a schema (structure or data skeleton), which involves defining the variable names as well the data types for each variable. The variable names and data types should match the original file that is being exported to BigQuery. Once the data is exported in a table, it can be previewed and queried through the 'compose query tab' on the left. Tables can additionally be joined given specification of a unique key. Past query and job history can be viewed on the left. 
  
  After the data is queried, the resulting dataset can be exported out to the Cloud. This export can be achieved by first creating a bucket from the Cloud console. Buckets can be created by selecting the Storage option from the tabbed main Cloud Platform console page. After a bucket is created, the queried data can be exported to that bucket with file name and format specified. 
  
  BigQuery has additional features worth highlighting like the publicly available datasets, which include the National Oceanic and Atmospheric Administration (NOAA) global data obtained from the USAF Climatology Center, US Disease Surveillance data from the CDC, NYC Taxi and Limousine Commission (TLC) Trip Data and GDELT Internet Archive Book Data. 
  

Datasets: The traffic fatalities dataset for 2015 was provided from the Fatality Analysis Reporting System (FARS). The original dataset contained about 19 different data files, all which highlighted the accident circumstances along with the driver and vehicle attributes. In our analysis, we analyzed the Person, Vechicle and Accident level data. The Person data contained one record per person and described all the individuals involved in the crash with information like age, sex, and injury severity. The Vehicle dataset contained one record per-in transport motor vehicle and describes information for all the vehicles involved in the accident like the vehicle's speed and deformed status. The Accident dataset contains one record per crash and holds information about crash attributes like the number of fatalities, and the environmental factors like the weather and lighting conditions. 

  The American Comminuty Survey (ACS) data was obtained through the ACS package via an api key (http://www.census.gov/data/developers/data-sets.html) . Data was collected for all states and counties in 2015 by specifying a key as well as the census level tables of interest. Since this dataset did not contain the FIPS encoding for states and counties, it was merged with the county and states data from the ACS package (specifying the FIPS encoding and the state and county names). Then a FIPS code column was created by multiplying the state code by 1000 and adding the county code to the product. ACS data was pulled from the Population and Income to Poverty Ratio in the Past 12 Months tables. Each of these datasets were merged with the county and state level data pulled earlier. Finally both income and poverty ratio tables were joined by the FIPS encoding column. An additional challenge from the data wrangling perspective was that Doña Ana County was encoded as Dona Ana County in the fips county and state datasets [from the ACS package]. The county name was converted to one form. 
  
  The FARS dataset was cleaned to remove any observations with unreported sex, reported drug use and death scence status update. Additionally a FIPS code column was created and used as the basis for joining the FARS and the ACS datasets so that each driver is matched with the county and state aggregate population and income to poverty ratio measures. Final predictors of interest as a measure of driver's drunk incidence included the driver's sex, age, history of previous DWI convictions and speeding related suspensions, driver's police reported drug use and death scene status, vehicle level attributes like speed before crash and degree of vehicle deformity following crash, accident level atributes like number of fatalities and county level variables like total population and income to poverty ratio in the past 12 months of the driver's state and county. 
  
  
Results/Analysis: Logistic regression was used first 

#Separate for predictors that increase odds of driver being drunk 

#Decrease odds of driver being drunk 
  
  
  
  
  
Format: You don’t need to follow a specific format in the write-up, but you should start with an introduc- tory paragraph and finish with a conclusion. These paragraphs needs not follow the formal writing style that you would use in most other classes. Here, a colloquial style that is accessible to a lay reader is appropriate.
Nevertheless, your write-up should address the following questions:
1. Why should anyone care about this?
2. What is this about? Do not assume that your readers have any domain knowledge! The burden of explanation as to what you are talking about is on you! For example, if your project involves phyllo- genetic trees, do not assume that your audience has anything other than a basic, lay understanding of genetics.
3. Where did your data come from? What kind of data was it? Is there a link to the data or some other way for the reader to follow up on your work?
4. What are your findings? What kind of statistical computations (if any) have you done to support those conclusions? Again, while the R code will show you performing the calculation, it is up to you to interpret, in English sentences, the results of these calculations. Do not forget about units, axis labels, etc.
￼4
Nicholas Horton STAT 495: Project Fall 2016
5. What are the limitations of your work? Be clear so that others do not misinterpret your findings. To what population do your results apply? Do they generalize? Could your work be extended with more data or computational power or time to analyze? How could your study be improved? Suggesting plausible extensions don’t weaken your work – they strengthen it by connecting it to future work.



Limitations and Future Work: 





```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(acs)
library(tibble)
library(dplyr)
library(mlbench)
library(caret)
library(readr)
require(mosaic)
library(plyr)
library(randomForest)
library(geepack)
library(gee)
```

```{r}
#Rejoining the state and county datasets to create a collective dataframe 
fips.county <- rename(fips.county, c("State" = "STATEL"))
fips.state <- rename(fips.state, c("STUSAB" = "STATEL"))
fips.county$STATEL <- as.factor(fips.county$STATEL)
fips.state$STATEL <- as.factor(fips.state$STATEL)
join <- inner_join(fips.state, fips.county, by = "STATEL")

fipsEncode <- join %>% mutate(FIPSCode = ((1000*State.ANSI) + County.ANSI))
fipsEncode1 <- fipsEncode %>% mutate(StateCounty = paste(County.Name, STATE_NAME, sep = ", "))

#TOTAL POPULATION 
x <- geo.make(state = "*", county = "*")
y <- acs.fetch(endyear=2015, geography=x, table.number="B01003", 
               key = "17b6e09794a8f4a42664535f0e519179cc06f5a7")
z <- estimate(y)
zdata <- as.data.frame(z)
dfPop <- tibble::rownames_to_column(zdata, "StateCounty")
JoinPop <- inner_join(dfPop, fipsEncode1, by = "StateCounty")

#INCOME TO POP RATIO  
x1 <- geo.make(state = "*", county = "*")
y1 <- acs.fetch(endyear=2015, geography=x1, table.number="C17002", key = "17b6e09794a8f4a42664535f0e519179cc06f5a7")
z1 <- estimate(y1)
z1 <- as.data.frame(z1)
z2 <- z1 %>% dplyr::select(C17002_001)
dfIncome <- tibble::rownames_to_column(z2, "StateCounty")
dfTotData <- inner_join(dfIncome, JoinPop, by = "StateCounty")

save(dfTotData, file = "dfTotData.csv")
```
